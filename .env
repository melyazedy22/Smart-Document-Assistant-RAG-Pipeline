# LLM Provider Configuration
# Options: 'openai', 'gemini', 'ollama', 'local', 'huggingface'
LLM_PROVIDER=groq

# 1. Hugging Face Configuration (Free API)
HUGGINGFACEHUB_API_TOKEN=your_key_here
# Repository ID for the model (e.g., mistralai/Mistral-7B-Instruct-v0.3, google/flan-t5-large)
HF_REPO_ID=HuggingFaceH4/zephyr-7b-beta

# 2. OpenAI Configuration (RECOMMENDED for stability)
# OPENAI_API_KEY=...

# 3. Google Gemini Configuration
GOOGLE_API_KEY=your_key_here

# 4. Groq Configuration
GROQ_API_KEY=your_key_here

# 3. Ollama Configuration (Free, Local)
# Install from https://ollama.com/
# Run: ollama run llama3
# LOCAL_LLM_URL=http://localhost:11434

# Vector Store Settings
VECTORSTORE_TYPE=faiss
VECTORSTORE_PATH=data/vectorstore
EMBEDDING_MODEL_NAME=all-MiniLM-L6-v2

# RAG Settings
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
TOP_K=4
FETCH_K=20
SCORE_THRESHOLD=0.5
