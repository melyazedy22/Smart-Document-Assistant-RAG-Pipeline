# ğŸ“‘ Smart Document Assistant

A local-first, privacy-focused Q&A assistant for long documents (contracts, insurance policies, reports). Built with **LangChain**, **RAG**, **Gradio**, and **FastAPI**.

Users upload PDF/DOCX files, the system extracts, chunks, and embeds content, stores it in a FAISS vector store, and enables chat-based question answering with **guardrails** and **source citations**.

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Gradio UI   â”‚    â”‚  FastAPI +   â”‚    â”‚      core.py         â”‚
â”‚ (frontend.py)â”‚â”€â”€â”€â–¶â”‚  LangServe   â”‚â”€â”€â”€â–¶â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚              â”‚    â”‚ (backend.py) â”‚    â”‚  â”‚   Config      â”‚   â”‚
â”‚ â€¢ Upload     â”‚    â”‚              â”‚    â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚ â€¢ Chat       â”‚    â”‚ Endpoints:   â”‚    â”‚  â”‚  Guardrails   â”‚   â”‚
â”‚ â€¢ Doc Select â”‚    â”‚  /rag        â”‚    â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚              â”‚    â”‚  /summarize  â”‚    â”‚  â”‚  Ingestion    â”‚   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”‚  Pipeline     â”‚   â”‚
                                        â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
                                        â”‚  â”‚ ChainBuilder  â”‚   â”‚
                                        â”‚  â”‚ (RAG + LLM)  â”‚   â”‚
                                        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
                                        â”‚         â”‚            â”‚
                                        â”‚    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”       â”‚
                                        â”‚    â”‚  FAISS  â”‚       â”‚
                                        â”‚    â”‚ Vector  â”‚       â”‚
                                        â”‚    â”‚  Store  â”‚       â”‚
                                        â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
                                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Main Components

| Component | File | Purpose |
|---|---|---|
| **Engine** | `core.py` | All logic: config, ingestion, embeddings, RAG chain, guardrails, summarization |
| **API** | `backend.py` | FastAPI + LangServe REST API (`/rag`, `/summarize`) |
| **UI** | `frontend.py` | Gradio dark-themed ChatGPT-style interface |
| **Notebooks** | `notebooks/` | Pipeline walkthrough + evaluation |

---

## ğŸ› ï¸ Technology Stack

| Category | Technology |
|---|---|
| **Framework** | LangChain, LangServe |
| **Backend** | FastAPI + Uvicorn |
| **Frontend** | Gradio |
| **Vector Store** | FAISS (default), Chroma (switchable) |
| **Embeddings** | HuggingFace SentenceTransformers (`all-MiniLM-L6-v2`) |
| **LLM Providers** | Groq (active), Gemini, OpenAI, HuggingFace, Ollama |
| **File Parsing** | PyMuPDF (PDF), Docx2txt (DOCX) |
| **Guardrails** | Custom embedding-based (input relevance + output grounding) |

---

## ğŸ“ Project Structure

```
Final_Project/
â”œâ”€â”€ core.py                  # Engine: Config, Ingestion, RAG, Guardrails
â”œâ”€â”€ backend.py               # FastAPI + LangServe API
â”œâ”€â”€ frontend.py              # Gradio UI
â”œâ”€â”€ .env                     # API keys & configuration
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ README.md                # This file
â”œâ”€â”€ DOCUMENTATION.md         # Detailed project documentation
â”œâ”€â”€ data/
â”‚   â””â”€â”€ vectorstore/         # FAISS index storage
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_System_Overview.ipynb   # Project spec & architecture
â”‚   â”œâ”€â”€ 02_RAG_Pipeline.ipynb      # Pipeline walkthrough
â”‚   â”œâ”€â”€ 03_Evaluation.ipynb        # Evaluation with metrics
â”‚   â””â”€â”€ evaluate.py                # LLM-as-Judge evaluator
â””â”€â”€ Project Discribtion/
    â”œâ”€â”€ Smart_Contract_Assistant_Spec.docx.pdf
    â””â”€â”€ LLM_Orchestration_Recap.pptx
```

---

## ğŸš€ Setup & Installation

### 1. Prerequisites
- Python 3.10+

### 2. Install Dependencies
```bash
pip install -r requirements.txt
```

### 3. Configure Environment
Create a `.env` file:
```ini
LLM_PROVIDER=groq              # Options: openai, gemini, groq, huggingface, ollama
GROQ_API_KEY=your_key_here     # Required if provider is groq
GOOGLE_API_KEY=your_key_here   # Required if provider is gemini
OPENAI_API_KEY=your_key_here   # Required if provider is openai

VECTORSTORE_TYPE=faiss
EMBEDDING_MODEL_NAME=all-MiniLM-L6-v2
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
TOP_K=4
FETCH_K=20
```

---

## ğŸ–¥ï¸ Usage

### Run the UI (Recommended)
```bash
python frontend.py
```
Open: `http://127.0.0.1:7860`

### Run the Backend API
```bash
python backend.py
```
- API Docs: `http://localhost:8000/docs`
- Playground: `http://localhost:8000/rag/playground`

---

## ğŸ”’ Guardrails

The system implements **embedding-based guardrails** for safety and factuality:

| Guardrail | Type | How It Works |
|---|---|---|
| **Input Check** | Safety | Uses FAISS similarity score to verify the query is relevant to indexed documents. Off-topic or harmful queries are blocked. |
| **Output Check** | Factuality | Uses cosine similarity between answer embedding and context embedding to verify the LLM answer is grounded in the documents. |

Both guardrails use configurable thresholds and follow a **fail-open** design (errors don't break the app).

---

## ğŸ“Š Evaluation

The project includes an evaluation pipeline (`notebooks/03_Evaluation.ipynb` + `notebooks/evaluate.py`):

| Metric | Description |
|---|---|
| **Faithfulness** | Is the answer grounded in the retrieved context? |
| **Relevance** | Does the answer address the user's question? |
| **Contains-Ground-Truth** | Does the answer contain the expected information? |

Evaluation uses an **LLM-as-Judge** approach where a secondary LLM scores the RAG pipeline's responses.

---

## âš ï¸ Known Limitations

1. **Embedding model runs on CPU** â€” Slower on large document sets. GPU would improve performance.
2. **No multi-language support** â€” Currently optimized for English documents only.
3. **No authentication** â€” Local deployment only, no user access control.
4. **Guardrail thresholds** â€” May need tuning per domain; current defaults work for general documents.
5. **Single-session history** â€” Chat history is not persisted across browser refreshes.
6. **FAISS in-memory** â€” Large vectorstores may consume significant RAM.
7. **No streaming** â€” Responses are returned as complete text, not streamed token-by-token.

---

## ğŸ”® Future Enhancements

- Multi-document cross-search
- Domain-specific fine-tuned models
- Role-based access control
- Cloud deployment (Docker/Kubernetes)
- Response streaming
- Persistent chat history
=======
# Smart-Document-Assistant-RAG-Pipeline
