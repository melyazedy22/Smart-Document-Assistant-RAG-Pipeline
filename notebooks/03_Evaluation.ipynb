{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ğŸ“Š Notebook 3 â€” RAG Pipeline Evaluation\n",
                "\n",
                "This notebook evaluates the **Smart Document Assistant** RAG pipeline across four dimensions:\n",
                "\n",
                "| Dimension | Method |\n",
                "|-----------|--------|\n",
                "| âœ… System Readiness | Checks that documents are ingested and chain is ready |\n",
                "| ğŸ” Retrieval Quality | Tests that relevant queries return non-empty answers |\n",
                "| ğŸ“ Answer Scoring | Fuzzy word-overlap score + exact contains check |\n",
                "| ğŸ›¡ï¸ Guardrails | Validates that input/output guardrails fire correctly |\n",
                "\n",
                "> **How to use this notebook:**\n",
                "> 1. First **ingest your documents** via the Gradio UI or `frontend.py`\n",
                "> 2. Update the `test_cases` list in Cell 3 with questions **from your actual documents**\n",
                "> 3. Run all cells top to bottom"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Cell 1 â€” Setup & Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "1eb7e526",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Working directory: e:\\ITI\\Level 2 (Advanced)\\Final_Project\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:core:Using HuggingFace embeddings: all-MiniLM-L6-v2\n",
                        "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
                        "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
                        "INFO:core:Reloading ChainBuilder...\n",
                        "INFO:faiss.loader:Loading faiss with AVX2 support.\n",
                        "INFO:faiss.loader:Successfully loaded faiss with AVX2 support.\n",
                        "INFO:core:Loaded FAISS vectorstore from data/vectorstore, total vectors: 11\n",
                        "INFO:core:Retriever created with k=20 (fetch_k for post-filter). Total vectors: 11\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… Imports successful\n",
                        "   LLM Provider  : groq\n",
                        "   Embedding Model: all-MiniLM-L6-v2\n",
                        "   Vectorstore   : e:\\ITI\\Level 2 (Advanced)\\Final_Project\\data\\vectorstore\n"
                    ]
                }
            ],
            "source": [
                "import sys\n",
                "import os\n",
                "import time\n",
                "import json\n",
                "\n",
                "# â”€â”€ Fix working directory â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "# core.py uses relative paths like 'data/vectorstore'.\n",
                "# When running from notebooks/, those paths break.\n",
                "# We must cd to the project root so everything resolves.\n",
                "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
                "os.chdir(PROJECT_ROOT)\n",
                "sys.path.insert(0, PROJECT_ROOT)\n",
                "print(f\"Working directory: {os.getcwd()}\")\n",
                "\n",
                "from core import (\n",
                "    get_rag_chain,\n",
                "    get_ingested_doc_names,\n",
                "    IngestionPipeline,\n",
                "    Guardrails,\n",
                "    Config\n",
                ")\n",
                "\n",
                "print(\"âœ… Imports successful\")\n",
                "print(f\"   LLM Provider  : {Config.LLM_PROVIDER}\")\n",
                "print(f\"   Embedding Model: {Config.EMBEDDING_MODEL_NAME}\")\n",
                "print(f\"   Vectorstore   : {os.path.abspath(Config.VECTORSTORE_PATH)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c7faa1ab",
            "metadata": {},
            "source": [
                "---\n",
                "## Cell 2 â€” System Readiness Check"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "b32248b8",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:core:Reloading ChainBuilder...\n",
                        "INFO:core:Loaded FAISS vectorstore from data/vectorstore, total vectors: 11\n",
                        "INFO:core:Retriever created with k=20 (fetch_k for post-filter). Total vectors: 11\n",
                        "INFO:core:Using HuggingFace embeddings: all-MiniLM-L6-v2\n",
                        "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
                        "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
                        "INFO:core:Loaded FAISS vectorstore from data/vectorstore, total vectors: 11\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "=======================================================\n",
                        "  SYSTEM READINESS CHECK\n",
                        "=======================================================\n",
                        "âœ… Documents indexed : 1\n",
                        "âœ… Total vectors     : 11\n",
                        "âœ… Chunk size        : 1000\n",
                        "âœ… Top-K retrieval   : 4\n",
                        "\n",
                        "  Ingested documents:\n",
                        "    1. Hatsuyuki Sakura reveiew .docx\n",
                        "=======================================================\n"
                    ]
                }
            ],
            "source": [
                "# Check what documents are currently ingested\n",
                "doc_names = get_ingested_doc_names()\n",
                "\n",
                "pipeline = IngestionPipeline()\n",
                "vectorstore = pipeline.load_vectorstore()\n",
                "\n",
                "print(\"=\" * 55)\n",
                "print(\"  SYSTEM READINESS CHECK\")\n",
                "print(\"=\" * 55)\n",
                "\n",
                "if not doc_names:\n",
                "    print(\"âŒ No documents found in the vectorstore!\")\n",
                "    print(\"   Please ingest documents first via frontend.py\")\n",
                "    print(\"   Then re-run this notebook.\")\n",
                "else:\n",
                "    total_vectors = vectorstore.index.ntotal if vectorstore and hasattr(vectorstore, 'index') else 'N/A'\n",
                "    print(f\"âœ… Documents indexed : {len(doc_names)}\")\n",
                "    print(f\"âœ… Total vectors     : {total_vectors}\")\n",
                "    print(f\"âœ… Chunk size        : {Config.CHUNK_SIZE}\")\n",
                "    print(f\"âœ… Top-K retrieval   : {Config.TOP_K}\")\n",
                "    print()\n",
                "    print(\"  Ingested documents:\")\n",
                "    for i, name in enumerate(doc_names, 1):\n",
                "        print(f\"    {i}. {name}\")\n",
                "\n",
                "print(\"=\" * 55)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5bb2b53c",
            "metadata": {},
            "source": [
                "---\n",
                "## Cell 3 â€” Define Your Test Cases\n",
                "\n",
                "> **âš ï¸ IMPORTANT**: Edit the `test_cases` list below with questions **relevant to your actual uploaded documents**.\n",
                ">\n",
                "> - `question` â†’ what you ask the RAG system\n",
                "> - `ground_truth` â†’ a key phrase or sentence you expect to appear in the answer  \n",
                "> - `doc_filter` â†’ *(optional)* restrict search to one specific document filename\n",
                ">\n",
                "> Leave `ground_truth` as `\"\"` if you just want to see the answer without scoring."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "af62a4b2",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… 3 test cases defined.\n",
                        "\n",
                        "  [1] Q: What is this document about?\n",
                        "       Expected: (no scoring â€” open-ended)\n",
                        "       Scope   : all documents\n",
                        "\n",
                        "  [2] Q: Summarize the main points of the document.\n",
                        "       Expected: (no scoring â€” open-ended)\n",
                        "       Scope   : all documents\n",
                        "\n",
                        "  [3] Q: What are the key terms or definitions mentioned?\n",
                        "       Expected: (no scoring â€” open-ended)\n",
                        "       Scope   : all documents\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# ============================================================\n",
                "# âœï¸  EDIT THESE TEST CASES TO MATCH YOUR DOCUMENTS\n",
                "# ============================================================\n",
                "test_cases = [\n",
                "    {\n",
                "        \"question\"    : \"What is this document about?\",\n",
                "        \"ground_truth\": \"\",          # Empty = no scoring, just show the answer\n",
                "        \"doc_filter\"  : None         # None = search all documents\n",
                "    },\n",
                "    {\n",
                "        \"question\"    : \"Summarize the main points of the document.\",\n",
                "        \"ground_truth\": \"\",\n",
                "        \"doc_filter\"  : None\n",
                "    },\n",
                "    {\n",
                "        \"question\"    : \"What are the key terms or definitions mentioned?\",\n",
                "        \"ground_truth\": \"\",\n",
                "        \"doc_filter\"  : None\n",
                "    },\n",
                "    # â”€â”€ Add your own questions below â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "    # {\n",
                "    #     \"question\"    : \"Your question here?\",\n",
                "    #     \"ground_truth\": \"A phrase you expect in the answer\",\n",
                "    #     \"doc_filter\"  : \"specific_file.pdf\"   # or None\n",
                "    # },\n",
                "]\n",
                "\n",
                "print(f\"âœ… {len(test_cases)} test cases defined.\")\n",
                "print()\n",
                "for i, tc in enumerate(test_cases, 1):\n",
                "    gt = tc['ground_truth'] if tc['ground_truth'] else \"(no scoring â€” open-ended)\"\n",
                "    filt = tc['doc_filter'] if tc['doc_filter'] else \"all documents\"\n",
                "    print(f\"  [{i}] Q: {tc['question']}\")\n",
                "    print(f\"       Expected: {gt}\")\n",
                "    print(f\"       Scope   : {filt}\")\n",
                "    print()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "edcceb63",
            "metadata": {},
            "source": [
                "---\n",
                "## Cell 4 â€” Helper: Scoring Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "c3793b59",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… Scoring functions ready\n",
                        "   - contains_score()      â†’ 1.0 if ground truth phrase found in answer\n",
                        "   - word_overlap_score()  â†’ fuzzy overlap (0.0â€“1.0)\n",
                        "   - answer_length_score() â†’ 1.0 if answer >= 10 words\n"
                    ]
                }
            ],
            "source": [
                "def contains_score(ground_truth: str, answer: str) -> float:\n",
                "    \"\"\"\n",
                "    Exact contains check (case-insensitive).\n",
                "    Returns 1.0 if ground_truth appears anywhere in answer, else 0.0.\n",
                "    \"\"\"\n",
                "    if not ground_truth:\n",
                "        return None  # No ground truth = not scored\n",
                "    return 1.0 if ground_truth.strip().lower() in answer.strip().lower() else 0.0\n",
                "\n",
                "\n",
                "def word_overlap_score(ground_truth: str, answer: str) -> float:\n",
                "    \"\"\"\n",
                "    Fuzzy word-overlap score (similar to ROUGE-1 recall).\n",
                "    Measures what fraction of ground-truth words appear in the answer.\n",
                "    Range: 0.0 â€“ 1.0\n",
                "    \"\"\"\n",
                "    if not ground_truth:\n",
                "        return None\n",
                "    gt_words  = set(ground_truth.lower().split())\n",
                "    ans_words = set(answer.lower().split())\n",
                "    if not gt_words:\n",
                "        return 0.0\n",
                "    overlap = gt_words & ans_words\n",
                "    return round(len(overlap) / len(gt_words), 3)\n",
                "\n",
                "\n",
                "def answer_length_score(answer: str, min_words: int = 10) -> float:\n",
                "    \"\"\"\n",
                "    Checks whether the answer is substantive (not empty or too short).\n",
                "    Returns 1.0 if answer has >= min_words words, else 0.0.\n",
                "    \"\"\"\n",
                "    words = answer.strip().split()\n",
                "    return 1.0 if len(words) >= min_words else 0.0\n",
                "\n",
                "\n",
                "print(\"âœ… Scoring functions ready\")\n",
                "print(\"   - contains_score()      â†’ 1.0 if ground truth phrase found in answer\")\n",
                "print(\"   - word_overlap_score()  â†’ fuzzy overlap (0.0â€“1.0)\")\n",
                "print(\"   - answer_length_score() â†’ 1.0 if answer >= 10 words\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8db9cb30",
            "metadata": {},
            "source": [
                "---\n",
                "## Cell 5 â€” Run Evaluation Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "4dd1c762",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:core:Reloading ChainBuilder...\n",
                        "INFO:core:Loaded FAISS vectorstore from data/vectorstore, total vectors: 11\n",
                        "INFO:core:Retriever created with k=20 (fetch_k for post-filter). Total vectors: 11\n",
                        "INFO:core:Guardrail input check: similarity=0.357, threshold=0.25, passed=True\n",
                        "INFO:core:Retrieved 11 raw documents for query: 'What is this document about?'\n",
                        "INFO:core:  Retrieved doc_names: ['Hatsuyuki Sakura reveiew .docx', 'Hatsuyuki Sakura reveiew .docx', 'Hatsuyuki Sakura reveiew .docx', 'Hatsuyuki Sakura reveiew .docx', 'Hatsuyuki Sakura reveiew .docx', 'Hatsuyuki Sakura reveiew .docx', 'Hatsuyuki Sakura reveiew .docx', 'Hatsuyuki Sakura reveiew .docx', 'Hatsuyuki Sakura reveiew .docx', 'Hatsuyuki Sakura reveiew .docx', 'Hatsuyuki Sakura reveiew .docx']\n",
                        "INFO:core:  After deduplication: 11 unique docs\n",
                        "INFO:core:  Final top_k=4 docs selected\n",
                        "INFO:core:    Doc 0: doc_name=Hatsuyuki Sakura reveiew .docx, page=0, chunk_id=Hatsuyuki Sakura reveiew .docx_chunk_2\n",
                        "INFO:core:    Doc 1: doc_name=Hatsuyuki Sakura reveiew .docx, page=0, chunk_id=Hatsuyuki Sakura reveiew .docx_chunk_7\n",
                        "INFO:core:    Doc 2: doc_name=Hatsuyuki Sakura reveiew .docx, page=0, chunk_id=Hatsuyuki Sakura reveiew .docx_chunk_0\n",
                        "INFO:core:    Doc 3: doc_name=Hatsuyuki Sakura reveiew .docx, page=0, chunk_id=Hatsuyuki Sakura reveiew .docx_chunk_1\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "============================================================\n",
                        "  RUNNING EVALUATION\n",
                        "============================================================\n",
                        "\n",
                        "[1/3] Question: What is this document about?\n",
                        "-------------------------------------------------------\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
                        "INFO:core:Guardrail output check: similarity=0.496, threshold=0.2, grounded=True\n",
                        "INFO:core:Reloading ChainBuilder...\n",
                        "INFO:core:Loaded FAISS vectorstore from data/vectorstore, total vectors: 11\n",
                        "INFO:core:Retriever created with k=20 (fetch_k for post-filter). Total vectors: 11\n",
                        "INFO:core:Guardrail input check: similarity=0.375, threshold=0.25, passed=True\n",
                        "INFO:core:Retrieved 11 raw documents for query: 'Summarize the main points of the document.'\n",
                        "INFO:core:  Retrieved doc_names: ['Hatsuyuki Sakura reveiew .docx', 'Hatsuyuki Sakura reveiew .docx', 'Hatsuyuki Sakura reveiew .docx', 'Hatsuyuki Sakura reveiew .docx', 'Hatsuyuki Sakura reveiew .docx', 'Hatsuyuki Sakura reveiew .docx', 'Hatsuyuki Sakura reveiew .docx', 'Hatsuyuki Sakura reveiew .docx', 'Hatsuyuki Sakura reveiew .docx', 'Hatsuyuki Sakura reveiew .docx', 'Hatsuyuki Sakura reveiew .docx']\n",
                        "INFO:core:  After deduplication: 11 unique docs\n",
                        "INFO:core:  Final top_k=4 docs selected\n",
                        "INFO:core:    Doc 0: doc_name=Hatsuyuki Sakura reveiew .docx, page=0, chunk_id=Hatsuyuki Sakura reveiew .docx_chunk_2\n",
                        "INFO:core:    Doc 1: doc_name=Hatsuyuki Sakura reveiew .docx, page=0, chunk_id=Hatsuyuki Sakura reveiew .docx_chunk_8\n",
                        "INFO:core:    Doc 2: doc_name=Hatsuyuki Sakura reveiew .docx, page=0, chunk_id=Hatsuyuki Sakura reveiew .docx_chunk_4\n",
                        "INFO:core:    Doc 3: doc_name=Hatsuyuki Sakura reveiew .docx, page=0, chunk_id=Hatsuyuki Sakura reveiew .docx_chunk_0\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  ğŸ“ Answer (34 words, 0.84s):\n",
                        "     This document appears to be a review of a visual novel or game called \"Hatsuyuki\n",
                        "     Sakura\". The review discusses the game's story, characters, and routes, providing\n",
                        "     an analysis of the game's strengths and weaknesses.\n",
                        "\n",
                        "  ğŸ“ Citations (4):\n",
                        "       - Hatsuyuki Sakura reveiew .docx (page 0)\n",
                        "       - Hatsuyuki Sakura reveiew .docx (page 0)\n",
                        "       - Hatsuyuki Sakura reveiew .docx (page 0)\n",
                        "       - Hatsuyuki Sakura reveiew .docx (page 0)\n",
                        "\n",
                        "  ğŸ›¡ï¸  Guardrails:\n",
                        "       Input  passed : True  | score: 0.3566383719444275\n",
                        "       Output grounded: True  | score: 0.49552791940586377\n",
                        "\n",
                        "  ğŸ“ Scores:\n",
                        "       Answer length OK      : âœ… (34 words)\n",
                        "       (no ground truth provided â€” open-ended question)\n",
                        "\n",
                        "[2/3] Question: Summarize the main points of the document.\n",
                        "-------------------------------------------------------\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
                        "INFO:core:Guardrail output check: similarity=0.589, threshold=0.2, grounded=True\n",
                        "INFO:core:Reloading ChainBuilder...\n",
                        "INFO:core:Loaded FAISS vectorstore from data/vectorstore, total vectors: 11\n",
                        "INFO:core:Retriever created with k=20 (fetch_k for post-filter). Total vectors: 11\n",
                        "INFO:core:Guardrail input check: similarity=0.359, threshold=0.25, passed=True\n",
                        "INFO:core:Retrieved 11 raw documents for query: 'What are the key terms or definitions mentioned?'\n",
                        "INFO:core:  Retrieved doc_names: ['Hatsuyuki Sakura reveiew .docx', 'Hatsuyuki Sakura reveiew .docx', 'Hatsuyuki Sakura reveiew .docx', 'Hatsuyuki Sakura reveiew .docx', 'Hatsuyuki Sakura reveiew .docx', 'Hatsuyuki Sakura reveiew .docx', 'Hatsuyuki Sakura reveiew .docx', 'Hatsuyuki Sakura reveiew .docx', 'Hatsuyuki Sakura reveiew .docx', 'Hatsuyuki Sakura reveiew .docx', 'Hatsuyuki Sakura reveiew .docx']\n",
                        "INFO:core:  After deduplication: 11 unique docs\n",
                        "INFO:core:  Final top_k=4 docs selected\n",
                        "INFO:core:    Doc 0: doc_name=Hatsuyuki Sakura reveiew .docx, page=0, chunk_id=Hatsuyuki Sakura reveiew .docx_chunk_10\n",
                        "INFO:core:    Doc 1: doc_name=Hatsuyuki Sakura reveiew .docx, page=0, chunk_id=Hatsuyuki Sakura reveiew .docx_chunk_2\n",
                        "INFO:core:    Doc 2: doc_name=Hatsuyuki Sakura reveiew .docx, page=0, chunk_id=Hatsuyuki Sakura reveiew .docx_chunk_4\n",
                        "INFO:core:    Doc 3: doc_name=Hatsuyuki Sakura reveiew .docx, page=0, chunk_id=Hatsuyuki Sakura reveiew .docx_chunk_0\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  ğŸ“ Answer (198 words, 1.09s):\n",
                        "     The document is a review of the visual novel \"Hatsuyuki Sakura\". Here are the main\n",
                        "     points summarized: 1. The game revolves around Kawano Hatsuyuki, who is revealed\n",
                        "     to be the Ghost Child, and his desire to avenge his beloved Ran's death. 2. The\n",
                        "     common route is well-written and sets up the groundwork for the story. 3. The game\n",
                        "     has 5 girls and 6 routes, with each route exploring a different character's relationship\n",
                        "     with Hatsuyuki. 4. The reviewer enjoyed Aya's route the most, finding it to be the\n",
                        "     best route in the game, with good plot relevancy and an interesting character. 5.\n",
                        "     The reviewer was disappointed with Shirokuma's route, finding it to be the shortest\n",
                        "     ... [118 more words]\n",
                        "\n",
                        "  ğŸ“ Citations (4):\n",
                        "       - Hatsuyuki Sakura reveiew .docx (page 0)\n",
                        "       - Hatsuyuki Sakura reveiew .docx (page 0)\n",
                        "       - Hatsuyuki Sakura reveiew .docx (page 0)\n",
                        "       - Hatsuyuki Sakura reveiew .docx (page 0)\n",
                        "\n",
                        "  ğŸ›¡ï¸  Guardrails:\n",
                        "       Input  passed : True  | score: 0.37479734420776367\n",
                        "       Output grounded: True  | score: 0.5885906087140488\n",
                        "\n",
                        "  ğŸ“ Scores:\n",
                        "       Answer length OK      : âœ… (198 words)\n",
                        "       (no ground truth provided â€” open-ended question)\n",
                        "\n",
                        "[3/3] Question: What are the key terms or definitions mentioned?\n",
                        "-------------------------------------------------------\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
                        "INFO:core:Guardrail output check: similarity=0.299, threshold=0.2, grounded=True\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  ğŸ“ Answer (144 words, 0.86s):\n",
                        "     Based on the provided context, the following key terms or definitions are mentioned:\n",
                        "     1. **Ghost Child**: Refers to Kawano Hatsuyuki, the main character, who is revealed\n",
                        "     to have this identity by the end of the common route. 2. **åé­‚é¦™ (Hankoukou)**: A\n",
                        "     term mentioned in the context, but its meaning is not explicitly defined. However,\n",
                        "     based on the context, it seems to be related to a concept or a plot element in the\n",
                        "     game. 3. **Soul Fragmentation**: A term mentioned in the context, which seems to\n",
                        "     be related to a plot element or a mechanic in the game. 4. **Progress Guidance Committee\n",
                        "     ( é€²è·¯æŒ‡å°å§”å“¡)**: A group mentioned in the context, which seems to be a part of the game's\n",
                        "     ... [64 more words]\n",
                        "\n",
                        "  ğŸ“ Citations (4):\n",
                        "       - Hatsuyuki Sakura reveiew .docx (page 0)\n",
                        "       - Hatsuyuki Sakura reveiew .docx (page 0)\n",
                        "       - Hatsuyuki Sakura reveiew .docx (page 0)\n",
                        "       - Hatsuyuki Sakura reveiew .docx (page 0)\n",
                        "\n",
                        "  ğŸ›¡ï¸  Guardrails:\n",
                        "       Input  passed : True  | score: 0.3592846393585205\n",
                        "       Output grounded: True  | score: 0.2987186360807531\n",
                        "\n",
                        "  ğŸ“ Scores:\n",
                        "       Answer length OK      : âœ… (144 words)\n",
                        "       (no ground truth provided â€” open-ended question)\n",
                        "\n",
                        "============================================================\n",
                        "  Evaluation complete â€” 3 questions processed\n",
                        "============================================================\n"
                    ]
                }
            ],
            "source": [
                "eval_results = []  # Stores all per-question results\n",
                "\n",
                "print(\"=\" * 60)\n",
                "print(\"  RUNNING EVALUATION\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "for i, case in enumerate(test_cases, 1):\n",
                "    question     = case[\"question\"]\n",
                "    ground_truth = case.get(\"ground_truth\", \"\")\n",
                "    doc_filter   = case.get(\"doc_filter\", None)\n",
                "\n",
                "    print(f\"\\n[{i}/{len(test_cases)}] Question: {question}\")\n",
                "    if doc_filter:\n",
                "        print(f\"       Doc filter: {doc_filter}\")\n",
                "    print(\"-\" * 55)\n",
                "\n",
                "    try:\n",
                "        # Build chain (with optional doc scope filter)\n",
                "        chain = get_rag_chain(doc_filter=doc_filter)\n",
                "\n",
                "        # Time the call\n",
                "        t_start  = time.time()\n",
                "        response = chain.invoke({\"question\": question})\n",
                "        elapsed  = round(time.time() - t_start, 2)\n",
                "\n",
                "        # Parse response\n",
                "        if isinstance(response, dict):\n",
                "            answer     = response.get(\"answer\", \"\")\n",
                "            citations  = response.get(\"citations\", [])\n",
                "            guardrails = response.get(\"guardrails\", {})\n",
                "            in_guard   = guardrails.get(\"input\", {})\n",
                "            out_guard  = guardrails.get(\"output\", {})\n",
                "        else:\n",
                "            answer     = str(response)\n",
                "            citations  = []\n",
                "            in_guard   = {}\n",
                "            out_guard  = {}\n",
                "\n",
                "        # Compute scores\n",
                "        c_score  = contains_score(ground_truth, answer)\n",
                "        wo_score = word_overlap_score(ground_truth, answer)\n",
                "        al_score = answer_length_score(answer)\n",
                "\n",
                "        # Print answer\n",
                "        print(f\"  ğŸ“ Answer ({len(answer.split())} words, {elapsed}s):\")\n",
                "        # Print answer wrapped at ~80 chars per line\n",
                "        words = answer.split()\n",
                "        line, lines = [], []\n",
                "        for w in words:\n",
                "            line.append(w)\n",
                "            if len(\" \".join(line)) > 78:\n",
                "                lines.append(\"     \" + \" \".join(line))\n",
                "                line = []\n",
                "        if line:\n",
                "            lines.append(\"     \" + \" \".join(line))\n",
                "        print(\"\\n\".join(lines[:8]))  # Show up to 8 lines\n",
                "        if len(lines) > 8:\n",
                "            print(f\"     ... [{len(words) - 80} more words]\")\n",
                "\n",
                "        # Print citations\n",
                "        if citations:\n",
                "            print(f\"\\n  ğŸ“ Citations ({len(citations)}):\")\n",
                "            for c in citations:\n",
                "                src  = c.source if hasattr(c, 'source') else c.get('source', '?')\n",
                "                page = c.page   if hasattr(c, 'page')   else c.get('page', '?')\n",
                "                print(f\"       - {src} (page {page})\")\n",
                "\n",
                "        # Print guardrails\n",
                "        print(f\"\\n  ğŸ›¡ï¸  Guardrails:\")\n",
                "        print(f\"       Input  passed : {in_guard.get('passed', 'N/A')}  | score: {in_guard.get('score', 'N/A')}\")\n",
                "        grounded = out_guard.get('grounded', 'N/A')\n",
                "        out_score = out_guard.get('score', 'N/A')\n",
                "        print(f\"       Output grounded: {grounded}  | score: {out_score}\")\n",
                "        if out_guard.get('warning'):\n",
                "            print(f\"       âš ï¸  {out_guard['warning']}\")\n",
                "\n",
                "        # Print scores\n",
                "        print(f\"\\n  ğŸ“ Scores:\")\n",
                "        print(f\"       Answer length OK      : {'âœ…' if al_score == 1.0 else 'âŒ'} ({len(answer.split())} words)\")\n",
                "        if c_score is not None:\n",
                "            print(f\"       Contains ground truth : {'âœ…' if c_score == 1.0 else 'âŒ'} ({c_score})\")\n",
                "            print(f\"       Word-overlap score    : {wo_score:.3f}  (ROUGE-1 recall)\")\n",
                "            if ground_truth:\n",
                "                print(f\"       Expected phrase       : '{ground_truth}'\")\n",
                "        else:\n",
                "            print(f\"       (no ground truth provided â€” open-ended question)\")\n",
                "\n",
                "        # Store result\n",
                "        eval_results.append({\n",
                "            \"question\"       : question,\n",
                "            \"answer\"         : answer,\n",
                "            \"ground_truth\"   : ground_truth,\n",
                "            \"doc_filter\"     : doc_filter,\n",
                "            \"elapsed_s\"      : elapsed,\n",
                "            \"num_citations\"  : len(citations),\n",
                "            \"input_passed\"   : in_guard.get('passed', None),\n",
                "            \"input_score\"    : in_guard.get('score', None),\n",
                "            \"output_grounded\": grounded,\n",
                "            \"output_score\"   : out_score,\n",
                "            \"contains_score\" : c_score,\n",
                "            \"word_overlap\"   : wo_score,\n",
                "            \"length_score\"   : al_score,\n",
                "            \"error\"          : None\n",
                "        })\n",
                "\n",
                "    except Exception as e:\n",
                "        print(f\"  âŒ ERROR: {e}\")\n",
                "        eval_results.append({\n",
                "            \"question\": question, \"answer\": \"\",\n",
                "            \"error\": str(e),\n",
                "            \"contains_score\": 0.0, \"word_overlap\": 0.0, \"length_score\": 0.0\n",
                "        })\n",
                "\n",
                "print()\n",
                "print(\"=\" * 60)\n",
                "print(f\"  Evaluation complete â€” {len(eval_results)} questions processed\")\n",
                "print(\"=\" * 60)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2d8611c6",
            "metadata": {},
            "source": [
                "---\n",
                "## Cell 6 â€” Summary Report"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "38eaa5da",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
                        "â•‘       ğŸ“Š  EVALUATION SUMMARY REPORT                â•‘\n",
                        "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
                        "â•‘  Total questions          : 3                        â•‘\n",
                        "â•‘  Successful invocations   : 3                        â•‘\n",
                        "â•‘  Errors                   : 0                        â•‘\n",
                        "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
                        "â•‘  RETRIEVAL & ANSWER QUALITY                        â•‘\n",
                        "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
                        "â•‘  Avg answer length score  : 1.0                      â•‘\n",
                        "â•‘  Contains-GT score        : N/A (no ground truths)   â•‘\n",
                        "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
                        "â•‘  GUARDRAILS                                        â•‘\n",
                        "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
                        "â•‘  Input guardrail pass rate: 100.0%                   â•‘\n",
                        "â•‘  Output grounding rate    : 100.0%                   â•‘\n",
                        "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
                        "â•‘  LATENCY                                           â•‘\n",
                        "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
                        "â•‘  Avg response time        : 0.93s                       â•‘\n",
                        "â•‘  Max response time        : 1.09s                       â•‘\n",
                        "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                        "\n",
                        "  Per-Question Results:\n",
                        "  #   Question (truncated)                 Length  Overlap   Time\n",
                        "  ---------------------------------------------------------------\n",
                        "  1   What is this document about?            1.0      N/A  0.84s\n",
                        "  2   Summarize the main points of the ..     1.0      N/A  1.09s\n",
                        "  3   What are the key terms or definit..     1.0      N/A  0.86s\n"
                    ]
                }
            ],
            "source": [
                "# â”€â”€ Aggregate Metrics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "total   = len(eval_results)\n",
                "errors  = sum(1 for r in eval_results if r.get(\"error\"))\n",
                "success = total - errors\n",
                "\n",
                "# Length scores (all questions)\n",
                "length_scores = [r[\"length_score\"] for r in eval_results if r.get(\"length_score\") is not None]\n",
                "avg_length_score = round(sum(length_scores) / len(length_scores), 3) if length_scores else 0.0\n",
                "\n",
                "# Contains / word-overlap (only scored questions)\n",
                "scored = [r for r in eval_results if r.get(\"contains_score\") is not None and r.get(\"ground_truth\")]\n",
                "if scored:\n",
                "    avg_contains  = round(sum(r[\"contains_score\"] for r in scored) / len(scored), 3)\n",
                "    avg_overlap   = round(sum(r[\"word_overlap\"]   for r in scored) / len(scored), 3)\n",
                "else:\n",
                "    avg_contains  = None\n",
                "    avg_overlap   = None\n",
                "\n",
                "# Latency\n",
                "latencies = [r[\"elapsed_s\"] for r in eval_results if r.get(\"elapsed_s\") and not r.get(\"error\")]\n",
                "avg_latency = round(sum(latencies) / len(latencies), 2) if latencies else 0.0\n",
                "max_latency = max(latencies) if latencies else 0.0\n",
                "\n",
                "# Input guardrail pass rate\n",
                "in_results = [r[\"input_passed\"] for r in eval_results if r.get(\"input_passed\") is not None]\n",
                "guard_pass_rate = round(sum(1 for v in in_results if v) / len(in_results), 3) if in_results else None\n",
                "\n",
                "# Output grounding rate\n",
                "out_results = [r[\"output_grounded\"] for r in eval_results if r.get(\"output_grounded\") is not None and r[\"output_grounded\"] != 'N/A']\n",
                "grounding_rate = round(sum(1 for v in out_results if v) / len(out_results), 3) if out_results else None\n",
                "\n",
                "# â”€â”€ Print Report â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "print()\n",
                "print(\"â•”\" + \"â•\" * 53 + \"â•—\")\n",
                "print(\"â•‘       ğŸ“Š  EVALUATION SUMMARY REPORT                â•‘\")\n",
                "print(\"â• \" + \"â•\" * 53 + \"â•£\")\n",
                "print(f\"â•‘  Total questions          : {total:<25}â•‘\")\n",
                "print(f\"â•‘  Successful invocations   : {success:<25}â•‘\")\n",
                "print(f\"â•‘  Errors                   : {errors:<25}â•‘\")\n",
                "print(\"â• \" + \"â•\" * 53 + \"â•£\")\n",
                "print(\"â•‘  RETRIEVAL & ANSWER QUALITY                        â•‘\")\n",
                "print(\"â• \" + \"â•\" * 53 + \"â•£\")\n",
                "print(f\"â•‘  Avg answer length score  : {avg_length_score:<25}â•‘\")\n",
                "if avg_contains is not None:\n",
                "    print(f\"â•‘  Contains-GT score (avg)  : {avg_contains:<25}â•‘\")\n",
                "    print(f\"â•‘  Word-overlap score (avg) : {avg_overlap:<25}â•‘\")\n",
                "    print(f\"â•‘  (based on {len(scored)} scored question(s))             â•‘\")\n",
                "else:\n",
                "    print(f\"â•‘  Contains-GT score        : N/A (no ground truths)   â•‘\")\n",
                "print(\"â• \" + \"â•\" * 53 + \"â•£\")\n",
                "print(\"â•‘  GUARDRAILS                                        â•‘\")\n",
                "print(\"â• \" + \"â•\" * 53 + \"â•£\")\n",
                "gr_str = f\"{grounding_rate:.1%}\" if grounding_rate is not None else \"N/A\"\n",
                "gp_str = f\"{guard_pass_rate:.1%}\" if guard_pass_rate is not None else \"N/A\"\n",
                "print(f\"â•‘  Input guardrail pass rate: {gp_str:<25}â•‘\")\n",
                "print(f\"â•‘  Output grounding rate    : {gr_str:<25}â•‘\")\n",
                "print(\"â• \" + \"â•\" * 53 + \"â•£\")\n",
                "print(\"â•‘  LATENCY                                           â•‘\")\n",
                "print(\"â• \" + \"â•\" * 53 + \"â•£\")\n",
                "print(f\"â•‘  Avg response time        : {avg_latency}s{'':<23}â•‘\")\n",
                "print(f\"â•‘  Max response time        : {max_latency}s{'':<23}â•‘\")\n",
                "print(\"â•š\" + \"â•\" * 53 + \"â•\")\n",
                "\n",
                "# Per-question table\n",
                "print()\n",
                "print(\"  Per-Question Results:\")\n",
                "print(f\"  {'#':<3} {'Question (truncated)':<35} {'Length':>7} {'Overlap':>8} {'Time':>6}\")\n",
                "print(\"  \" + \"-\" * 63)\n",
                "for i, r in enumerate(eval_results, 1):\n",
                "    q_short   = r['question'][:33] + \"..\" if len(r['question']) > 33 else r['question']\n",
                "    ls        = f\"{r['length_score']:.1f}\" if r.get('length_score') is not None else \"err\"\n",
                "    wo        = f\"{r['word_overlap']:.3f}\" if r.get('word_overlap') is not None else \" N/A\"\n",
                "    t         = f\"{r['elapsed_s']}s\"        if r.get('elapsed_s')   else \"err\"\n",
                "    print(f\"  {i:<3} {q_short:<35} {ls:>7} {wo:>8} {t:>6}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "30cb8973",
            "metadata": {},
            "source": [
                "---\n",
                "## Cell 7 â€” Guardrail Stress Test\n",
                "\n",
                "Directly tests the guardrail mechanisms with known on-topic and off-topic queries."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "9ab8a530",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:core:Using HuggingFace embeddings: all-MiniLM-L6-v2\n",
                        "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
                        "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
                        "INFO:core:Loaded FAISS vectorstore from data/vectorstore, total vectors: 11\n",
                        "INFO:core:Guardrail input check: similarity=0.357, threshold=0.25, passed=True\n",
                        "INFO:core:Guardrail input check: similarity=0.398, threshold=0.25, passed=True\n",
                        "INFO:core:Guardrail input check: similarity=0.366, threshold=0.25, passed=True\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "============================================================\n",
                        "  ğŸ›¡ï¸  GUARDRAIL STRESS TEST\n",
                        "============================================================\n",
                        "  âœ…  [On-topic (general)]\n",
                        "       Query   : What is this document about?\n",
                        "       Score   : 0.3566  |  Passed: True  |  Expected: True\n",
                        "\n",
                        "  âœ…  [On-topic (specific)]\n",
                        "       Query   : What are the main points mentioned?\n",
                        "       Score   : 0.3975  |  Passed: True  |  Expected: True\n",
                        "\n",
                        "  âŒ  [Off-topic (food)]\n",
                        "       Query   : What is the recipe for chocolate cake with vanilla frosting?\n",
                        "       Score   : 0.3662  |  Passed: True  |  Expected: False\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:core:Guardrail input check: similarity=0.351, threshold=0.25, passed=True\n",
                        "INFO:core:Guardrail input check: similarity=0.357, threshold=0.25, passed=True\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  âŒ  [Off-topic (weather)]\n",
                        "       Query   : What is the weather forecast for Cairo tomorrow?\n",
                        "       Score   : 0.3509  |  Passed: True  |  Expected: False\n",
                        "\n",
                        "  âŒ  [Gibberish]\n",
                        "       Query   : asdfghjkl qwerty zxcvbnm\n",
                        "       Score   : 0.3573  |  Passed: True  |  Expected: False\n",
                        "\n",
                        "  Guardrail accuracy: 2/5 (40%)\n",
                        "============================================================\n"
                    ]
                }
            ],
            "source": [
                "pipeline    = IngestionPipeline()\n",
                "vectorstore = pipeline.load_vectorstore()\n",
                "embeddings  = pipeline.embeddings\n",
                "\n",
                "guardrail_tests = [\n",
                "    # (label, query, expect_to_pass)\n",
                "    (\"On-topic (general)\",  \"What is this document about?\",                                True),\n",
                "    (\"On-topic (specific)\", \"What are the main points mentioned?\",                         True),\n",
                "    (\"Off-topic (food)\",    \"What is the recipe for chocolate cake with vanilla frosting?\", False),\n",
                "    (\"Off-topic (weather)\", \"What is the weather forecast for Cairo tomorrow?\",             False),\n",
                "    (\"Gibberish\",           \"asdfghjkl qwerty zxcvbnm\",                                    False),\n",
                "]\n",
                "\n",
                "print(\"=\" * 60)\n",
                "print(\"  ğŸ›¡ï¸  GUARDRAIL STRESS TEST\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "if not vectorstore:\n",
                "    print(\"âŒ No vectorstore â€” skipping guardrail tests.\")\n",
                "else:\n",
                "    guard_results = []\n",
                "    for label, query, expect_pass in guardrail_tests:\n",
                "        result  = Guardrails.input_check(query, vectorstore, embeddings)\n",
                "        passed  = result.get(\"passed\", False)\n",
                "        score   = result.get(\"score\",  0.0)\n",
                "        correct = (passed == expect_pass)\n",
                "        status  = \"âœ…\" if correct else \"âŒ\"\n",
                "        print(f\"  {status}  [{label}]\")\n",
                "        print(f\"       Query   : {query[:60]}\")\n",
                "        print(f\"       Score   : {score:.4f}  |  Passed: {passed}  |  Expected: {expect_pass}\")\n",
                "        if result.get(\"message\"):\n",
                "            print(f\"       Message : {result['message'][:80]}\")\n",
                "        print()\n",
                "        guard_results.append(correct)\n",
                "\n",
                "    correct_count = sum(guard_results)\n",
                "    print(f\"  Guardrail accuracy: {correct_count}/{len(guard_results)} ({100*correct_count//len(guard_results)}%)\")\n",
                "\n",
                "print(\"=\" * 60)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0d5578b0",
            "metadata": {},
            "source": [
                "---\n",
                "## Cell 8 â€” Save Results to JSON"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "89aa9ac9",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… Results saved to: e:\\ITI\\Level 2 (Advanced)\\Final_Project\\data\\eval_results\\eval_20260219_020211.json\n",
                        "   File size: 4029 bytes\n"
                    ]
                }
            ],
            "source": [
                "import datetime\n",
                "\n",
                "output_dir = os.path.join(\"data\", \"eval_results\")\n",
                "os.makedirs(output_dir, exist_ok=True)\n",
                "\n",
                "timestamp   = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
                "output_path = os.path.join(output_dir, f\"eval_{timestamp}.json\")\n",
                "\n",
                "report = {\n",
                "    \"timestamp\"          : timestamp,\n",
                "    \"llm_provider\"       : Config.LLM_PROVIDER,\n",
                "    \"embedding_model\"    : Config.EMBEDDING_MODEL_NAME,\n",
                "    \"documents_indexed\"  : doc_names,\n",
                "    \"total_questions\"    : total,\n",
                "    \"avg_length_score\"   : avg_length_score,\n",
                "    \"avg_contains_score\" : avg_contains,\n",
                "    \"avg_word_overlap\"   : avg_overlap,\n",
                "    \"avg_latency_s\"      : avg_latency,\n",
                "    \"max_latency_s\"      : max_latency,\n",
                "    \"guardrail_pass_rate\": guard_pass_rate,\n",
                "    \"output_grounding_rate\": grounding_rate,\n",
                "    \"questions\"          : eval_results\n",
                "}\n",
                "\n",
                "# Make result serializable\n",
                "def make_serializable(obj):\n",
                "    if isinstance(obj, dict):\n",
                "        return {k: make_serializable(v) for k, v in obj.items()}\n",
                "    elif isinstance(obj, list):\n",
                "        return [make_serializable(v) for v in obj]\n",
                "    elif hasattr(obj, '__dict__'):\n",
                "        return make_serializable(obj.__dict__)\n",
                "    else:\n",
                "        try:\n",
                "            json.dumps(obj)\n",
                "            return obj\n",
                "        except:\n",
                "            return str(obj)\n",
                "\n",
                "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
                "    json.dump(make_serializable(report), f, indent=2, ensure_ascii=False)\n",
                "\n",
                "print(f\"âœ… Results saved to: {os.path.abspath(output_path)}\")\n",
                "print(f\"   File size: {os.path.getsize(output_path)} bytes\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## ğŸ“‹ Metric Reference\n",
                "\n",
                "| Metric | How It's Computed | Good Score |\n",
                "|--------|------------------|------------|\n",
                "| **Answer Length Score** | `1.0` if answer â‰¥ 10 words, else `0.0` | `1.0` |\n",
                "| **Contains-GT Score** | `1.0` if ground-truth phrase found inside answer | `1.0` |\n",
                "| **Word-Overlap Score** | Fraction of ground-truth words that appear in answer (ROUGE-1 recall) | `> 0.5` |\n",
                "| **Input Guardrail Score** | `1 / (1 + L2_distance)` vs the nearest FAISS chunk | `> 0.25` |\n",
                "| **Output Grounding Score** | Cosine similarity between `embed(answer)` and `embed(context)` | `> 0.20` |\n",
                "| **Response Time** | Wall-clock seconds from `chain.invoke()` to result | `< 10s` |\n",
                "\n",
                "---\n",
                "\n",
                "## âš ï¸ Limitations of This Evaluation\n",
                "\n",
                "1. **String-match scoring is brittle** â€” a perfect rephrased answer scores 0.0 if no exact words overlap.\n",
                "2. **No semantic scoring** â€” for true semantic evaluation, consider integrating [RAGAS](https://github.com/explodinggradients/ragas) or an LLM judge.\n",
                "3. **Ground truth must be hand-curated** â€” there is no automatic Q/A generation in this notebook.\n",
                "4. **Single-turn only** â€” multi-turn conversation quality is not evaluated here."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbformat_minor": 5,
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
